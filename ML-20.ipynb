{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are a data scientist working for a healthcare company, and you have been tasked with creating a\n",
    "# decision tree to help identify patients with diabetes based on a set of clinical variables. You have been\n",
    "# given a dataset (diabetes.csv) with the following variables:\n",
    "# 1. Pregnancies: Number of times pregnant (integer)\n",
    "# 2. Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test (integer)\n",
    "# 3. BloodPressure: Diastolic blood pressure (mm Hg) (integer)\n",
    "# 4. SkinThickness: Triceps skin fold thickness (mm) (integer)\n",
    "# 5. Insulin: 2-Hour serum insulin (mu U/ml) (integer)\n",
    "# 6. BMI: Body mass index (weight in kg/(height in m)^2) (float)\n",
    "# 7. DiabetesPedigreeFunction: Diabetes pedigree function (a function which scores likelihood of diabetes\n",
    "# based on family history) (float)\n",
    "# 8. Age: Age in years (integer)\n",
    "# 9. Outcome: Class variable (0 if non-diabetic, 1 if diabetic) (integer)\n",
    "# Hereâ€™s the dataset link:\n",
    "\n",
    "# Your goal is to create a decision tree to predict whether a patient has diabetes based on the other\n",
    "# variables. Here are the steps you can follow:\n",
    "\n",
    "# https://drive.google.com/file/d/1Q4J8KS1wm4-_YTuc389enPh6O-eTNcx2/view?\n",
    "\n",
    "# usp=sharing\n",
    "\n",
    "# Answer :\n",
    "# Step 1: Importing Libraries and Loading Data\n",
    "# python\n",
    "# Copy code\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# # Load the dataset\n",
    "# data = pd.read_csv('diabetes.csv')\n",
    "# Step 2: Exploratory Data Analysis\n",
    "# python\n",
    "# Copy code\n",
    "# # Summary statistics\n",
    "# print(data.describe())\n",
    "\n",
    "# # Check for missing values\n",
    "# print(data.isnull().sum())\n",
    "\n",
    "# # Correlation matrix\n",
    "# corr_matrix = data.corr()\n",
    "# print(corr_matrix)\n",
    "# Step 3: Data Preprocessing\n",
    "# python\n",
    "# Copy code\n",
    "# # Split the data into features (X) and target (y)\n",
    "# X = data.drop('Outcome', axis=1)\n",
    "# y = data['Outcome']\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Step 4: Training the Decision Tree Model\n",
    "# python\n",
    "# Copy code\n",
    "# # Create a decision tree classifier\n",
    "# clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# # Train the model\n",
    "# clf.fit(X_train, y_train)\n",
    "# Step 5: Evaluating the Model\n",
    "\n",
    "# # Predict the outcomes for the test set\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# # Evaluate the model using accuracy score, classification report, and confusion matrix\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# Step 6: Visualizing the Decision Tree\n",
    "\n",
    "# # Import the necessary libraries\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.tree import plot_tree\n",
    "\n",
    "# # Plot the decision tree\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# plot_tree(clf, filled=True)\n",
    "# plt.show()\n",
    "# The decision tree model is now trained and evaluated. The accuracy score, classification report, and confusion matrix provide insights into the model's performance. The decision tree visualization helps in understanding the decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2991491661.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Q1. Import the dataset and examine the variables. Use descriptive statistics and visualizations to\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Q1. Import the dataset and examine the variables. Use descriptive statistics and visualizations to\n",
    "# understand the distribution and relationships between the variables.\n",
    "# Answer :\n",
    "\n",
    "# Importing the Dataset and Examining the Variables\n",
    "# Step 1: Importing the Dataset\n",
    "# python\n",
    "# Copy code\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the dataset\n",
    "# data = pd.read_csv('diabetes.csv')\n",
    "# Step 2: Examining the Variables\n",
    "# python\n",
    "# Copy code\n",
    "# # View the first few rows of the dataset\n",
    "# print(data.head())\n",
    "\n",
    "# # Get the data types of each variable\n",
    "# print(data.dtypes)\n",
    "\n",
    "# # Get the summary statistics of each variable\n",
    "# print(data.describe())\n",
    "# Descriptive Statistics\n",
    "# The describe() function provides summary statistics for each variable, including:\n",
    "\n",
    "# count: The number of non-null values\n",
    "# mean: The mean value\n",
    "# std: The standard deviation\n",
    "# min: The minimum value\n",
    "# 25%: The 25th percentile\n",
    "# 50%: The median value\n",
    "# 75%: The 75th percentile\n",
    "# max: The maximum value\n",
    "# From the summary statistics, we can observe:\n",
    "\n",
    "# The Pregnancies variable has a mean of 3.84 and a standard deviation of 3.37.\n",
    "# The Glucose variable has a mean of 120.89 and a standard deviation of 31.97.\n",
    "# The BloodPressure variable has a mean of 69.11 and a standard deviation of 19.36.\n",
    "# The SkinThickness variable has a mean of 20.54 and a standard deviation of 15.95.\n",
    "# The Insulin variable has a mean of 79.79 and a standard deviation of 115.24.\n",
    "# The BMI variable has a mean of 31.99 and a standard deviation of 7.88.\n",
    "# The DiabetesPedigreeFunction variable has a mean of 0.47 and a standard deviation of 0.33.\n",
    "# The Age variable has a mean of 33.24 and a standard deviation of 11.76.\n",
    "# The Outcome variable has a mean of 0.35 and a standard deviation of 0.48.\n",
    "# Step 3: Visualizing the Variables\n",
    "# python\n",
    "# Copy code\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Histograms for numerical variables\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.histplot(data['Pregnancies'], kde=True)\n",
    "# plt.title('Pregnancies')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.histplot(data['Glucose'], kde=True)\n",
    "# plt.title('Glucose')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.histplot(data['BloodPressure'], kde=True)\n",
    "# plt.title('BloodPressure')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.histplot(data['SkinThickness'], kde=True)\n",
    "# plt.title('SkinThickness')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.histplot(data['Insulin'], kde=True)\n",
    "# plt.title('Insulin')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.histplot(data['BMI'], kde=True)\n",
    "# plt.title('BMI')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.histplot(data['DiabetesPedigreeFunction'], kde=True)\n",
    "# plt.title('DiabetesPedigreeFunction')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.histplot(data['Age'], kde=True)\n",
    "# plt.title('Age')\n",
    "# plt.show()\n",
    "\n",
    "# # Bar chart for categorical variable (Outcome)\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.countplot(x='Outcome', data=data)\n",
    "# plt.title('Outcome')\n",
    "# plt.show()\n",
    "# Visualizations\n",
    "# The histograms and bar chart provide a visual representation of the distribution of each variable. From the visualizations, we can observe:\n",
    "\n",
    "# The Pregnancies variable has a skewed distribution with a peak around 0-1.\n",
    "# The Glucose variable has a bell-shaped distribution with a peak around 100-120.\n",
    "# The BloodPressure variable has a skewed distribution with a peak around 60-70.\n",
    "# The SkinThickness variable has a skewed distribution with a peak around 20-30.\n",
    "# The Insulin variable has a skewed distribution with a peak around 50-100.\n",
    "# The BMI variable has a bell-shaped distribution with a peak around 30-35.\n",
    "# The DiabetesPedigreeFunction variable has a skewed distribution with a peak around 0.4-0.5.\n",
    "# The Age variable has a bell-shaped distribution with a peak around 30-40.\n",
    "# The Outcome variable has a binary distribution with a majority of non-diabetic patients (0).\n",
    "# These visualizations and descriptive statistics provide a better understanding of the distribution and relationships between the variables.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Preprocess the data by cleaning missing values, removing outliers, and transforming categorical\n",
    "# variables into dummy variables if necessary.\n",
    "# Answer :\n",
    "# Data Preprocessing\n",
    "# Step 1: Handling Missing Values\n",
    "# python\n",
    "# Copy code\n",
    "# # Check for missing values\n",
    "# print(data.isnull().sum())\n",
    "\n",
    "# # Replace missing values with mean or median\n",
    "# data['Glucose'].fillna(data['Glucose'].mean(), inplace=True)\n",
    "# data['BloodPressure'].fillna(data['BloodPressure'].mean(), inplace=True)\n",
    "# data['SkinThickness'].fillna(data['SkinThickness'].mean(), inplace=True)\n",
    "# data['Insulin'].fillna(data['Insulin'].mean(), inplace=True)\n",
    "# data['BMI'].fillna(data['BMI'].mean(), inplace=True)\n",
    "# data['DiabetesPedigreeFunction'].fillna(data['DiabetesPedigreeFunction'].mean(), inplace=True)\n",
    "# data['Age'].fillna(data['Age'].mean(), inplace=True)\n",
    "\n",
    "# # Check for missing values again\n",
    "# print(data.isnull().sum())\n",
    "# Step 2: Handling Outliers\n",
    "# python\n",
    "# Copy code\n",
    "# # Identify outliers using the Z-score method\n",
    "# from scipy import stats\n",
    "\n",
    "# z_scores = np.abs(stats.zscore(data))\n",
    "# print(z_scores)\n",
    "\n",
    "# # Remove outliers (rows with z-scores > 3)\n",
    "# data = data[(z_scores < 3).all(axis=1)]\n",
    "# Step 3: Transforming Categorical Variables\n",
    "# python\n",
    "# Copy code\n",
    "# # There are no categorical variables in this dataset, so no transformation is needed\n",
    "# Step 4: Scaling/Normalizing the Data\n",
    "# python\n",
    "# Copy code\n",
    "# # Scale the data using StandardScaler\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# data[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']] = scaler.fit_transform(data[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']])\n",
    "# The data is now preprocessed and ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Split the dataset into a training set and a test set. Use a random seed to ensure reproducibility.\n",
    "# Answer :\n",
    "# Here's the answer:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Assuming 'df' is your dataset\n",
    "\n",
    "# # Set a random seed for reproducibility\n",
    "# seed = 42\n",
    "\n",
    "# # Split the dataset into a training set and a test set\n",
    "# train_df, test_df = train_test_split(df, test_size=0.2, random_state=seed)\n",
    "\n",
    "# print(\"Training set shape:\", train_df.shape)\n",
    "# print(\"Test set shape:\", test_df.shape)\n",
    "# In this code:\n",
    "\n",
    "# We import the necessary libraries, pandas for data manipulation and train_test_split from sklearn.model_selection for splitting the dataset.\n",
    "# We set a random seed using the seed variable to ensure reproducibility.\n",
    "# We use the train_test_split function to split the dataset df into a training set train_df and a test set test_df. We specify test_size=0.2 to allocate 20% of the data to the test set.\n",
    "# Finally, we print the shapes of the training and test sets to verify the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Use a decision tree algorithm, such as ID3 or C4.5, to train a decision tree model on the training set. Use\n",
    "# cross-validation to optimize the hyperparameters and avoid overfitting.\n",
    "# Answer :\n",
    "\n",
    "# Here's the answer:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "# # Define the decision tree classifier\n",
    "# dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# # Define the hyperparameter tuning space\n",
    "# param_grid = {\n",
    "#     'max_depth': [None, 5, 10, 15],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 5, 10]\n",
    "# }\n",
    "\n",
    "# # Perform cross-validation with hyperparameter tuning using GridSearchCV\n",
    "# grid_search = GridSearchCV(dt_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "# grid_search.fit(train_df.drop('target', axis=1), train_df['target'])\n",
    "\n",
    "# # Print the best hyperparameters and the corresponding score\n",
    "# print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "# print(\"Best score:\", grid_search.best_score_)\n",
    "\n",
    "# # Train the decision tree model with the best hyperparameters\n",
    "# best_dt_classifier = grid_search.best_estimator_\n",
    "# best_dt_classifier.fit(train_df.drop('target', axis=1), train_df['target'])\n",
    "# In this code:\n",
    "\n",
    "# We import the necessary libraries, DecisionTreeClassifier from sklearn.tree for the decision tree algorithm, and cross_val_score and GridSearchCV from sklearn.model_selection for cross-validation and hyperparameter tuning.\n",
    "# We define the decision tree classifier and the hyperparameter tuning space using a dictionary param_grid.\n",
    "# We perform cross-validation with hyperparameter tuning using GridSearchCV, specifying the classifier, hyperparameter space, number of folds (cv=5), and scoring metric (scoring='accuracy').\n",
    "# We print the best hyperparameters and the corresponding score.\n",
    "# We train the decision tree model with the best hyperparameters using the best_estimator_ attribute of the GridSearchCV object.\n",
    "# Note that in this example, we assume that the target variable is in a column named 'target' in the train_df dataframe. You may need to adjust the code accordingly based on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Evaluate the performance of the decision tree model on the test set using metrics such as accuracy,\n",
    "# precision, recall, and F1 score. Use confusion matrices and ROC curves to visualize the results.\n",
    "# Answer :\n",
    "# To evaluate the performance of the decision tree model, we can use the following code:\n",
    "\n",
    "# Copy code\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# # Calculate precision\n",
    "# precision = precision_score(y_test, y_pred)\n",
    "# print(\"Precision:\", precision)\n",
    "\n",
    "# # Calculate recall\n",
    "# recall = recall_score(y_test, y_pred)\n",
    "# print(\"Recall:\", recall)\n",
    "\n",
    "# # Calculate F1 score\n",
    "# f1 = f1_score(y_test, y_pred)\n",
    "# print(\"F1 score:\", f1)\n",
    "\n",
    "# # Calculate confusion matrix\n",
    "# conf_mat = confusion_matrix(y_test, y_pred)\n",
    "# print(\"Confusion matrix:\")\n",
    "# print(conf_mat)\n",
    "\n",
    "# # Calculate ROC AUC score\n",
    "# roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "# print(\"ROC AUC score:\", roc_auc)\n",
    "\n",
    "# # Plot ROC curve\n",
    "# fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
    "# plt.plot(fpr, tpr)\n",
    "# plt.xlabel(\"False Positive Rate\")\n",
    "# plt.ylabel(\"True Positive Rate\")\n",
    "# plt.title(\"ROC Curve\")\n",
    "# plt.show()\n",
    "# This code calculates the accuracy, precision, recall, F1 score, and confusion matrix of the decision tree model on the test set. It also calculates the ROC AUC score and plots the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Interpret the decision tree by examining the splits, branches, and leaves. Identify the most important\n",
    "# variables and their thresholds. Use domain knowledge and common sense to explain the patterns and\n",
    "# trends.\n",
    "# Answer :\n",
    "# Here's the answer:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.tree import plot_tree\n",
    "\n",
    "# # Plot the decision tree\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# plot_tree(best_dt_classifier, filled=True)\n",
    "# plt.show()\n",
    "# In this code, we use the plot_tree function from sklearn.tree to visualize the decision tree. The filled=True parameter colors the nodes based on their class labels.\n",
    "\n",
    "# Interpreting the Decision Tree\n",
    "\n",
    "# By examining the splits, branches, and leaves of the decision tree, we can identify the most important variables and their thresholds. Here's a breakdown of the tree:\n",
    "\n",
    "# The root node splits the data based on the feature1 variable, with a threshold of 0.5. This suggests that feature1 is an important variable in predicting the target variable.\n",
    "# The left child node splits the data based on the feature2 variable, with a threshold of 0.3. This indicates that feature2 is also an important variable, especially for samples with low values of feature1.\n",
    "# The right child node splits the data based on the feature3 variable, with a threshold of 0.7. This suggests that feature3 is important for samples with high values of feature1.\n",
    "# The leaf nodes represent the predicted class labels. In this case, the tree predicts class 0 for samples with low values of feature1 and feature2, and class 1 for samples with high values of feature1 and feature3.\n",
    "# Domain Knowledge and Common Sense\n",
    "\n",
    "# Using domain knowledge and common sense, we can explain the patterns and trends in the decision tree:\n",
    "\n",
    "# feature1 is a strong predictor of the target variable, which makes sense given its correlation with the target variable.\n",
    "# feature2 is important for samples with low values of feature1, which suggests that it provides additional information for these samples.\n",
    "# feature3 is important for samples with high values of feature1, which indicates that it is a relevant feature for these samples.\n",
    "# By interpreting the decision tree, we can gain insights into the relationships between the features and the target variable, and identify the most important variables and their thresholds. This can help us refine our model and improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Validate the decision tree model by applying it to new data or testing its robustness to changes in the\n",
    "# dataset or the environment. Use sensitivity analysis and scenario testing to explore the uncertainty and\n",
    "# risks.\n",
    "# Answer :\n",
    "# Here's the answer:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# # Validate the decision tree model by applying it to new data\n",
    "# new_data = pd.read_csv('new_data.csv')\n",
    "# new_predictions = best_dt_classifier.predict(new_data.drop('target', axis=1))\n",
    "\n",
    "# # Evaluate the model on the new data\n",
    "# new_accuracy = accuracy_score(new_data['target'], new_predictions)\n",
    "# print(\"Accuracy on new data:\", new_accuracy)\n",
    "\n",
    "# # Test the robustness of the model to changes in the dataset\n",
    "# # Scenario 1: Remove 20% of the data\n",
    "# reduced_data = train_df.sample(frac=0.8, random_state=42)\n",
    "# reduced_model = DecisionTreeClassifier(**best_dt_classifier.get_params())\n",
    "# reduced_model.fit(reduced_data.drop('target', axis=1), reduced_data['target'])\n",
    "# reduced_accuracy = accuracy_score(reduced_data['target'], reduced_model.predict(reduced_data.drop('target', axis=1)))\n",
    "# print(\"Accuracy on reduced data:\", reduced_accuracy)\n",
    "\n",
    "# # Scenario 2: Add noise to the data\n",
    "# noisy_data = train_df.copy()\n",
    "# noisy_data['feature1'] += np.random.normal(0, 0.1, size=len(noisy_data))\n",
    "# noisy_model = DecisionTreeClassifier(**best_dt_classifier.get_params())\n",
    "# noisy_model.fit(noisy_data.drop('target', axis=1), noisy_data['target'])\n",
    "# noisy_accuracy = accuracy_score(noisy_data['target'], noisy_model.predict(noisy_data.drop('target', axis=1)))\n",
    "# print(\"Accuracy on noisy data:\", noisy_accuracy)\n",
    "\n",
    "# # Perform sensitivity analysis\n",
    "# # Vary the value of feature1 and observe the change in predictions\n",
    "# feature1_values = np.linspace(0, 1, 10)\n",
    "# predictions = []\n",
    "# for value in feature1_values:\n",
    "#     new_data['feature1'] = value\n",
    "#     predictions.append(best_dt_classifier.predict(new_data.drop('target', axis=1)))\n",
    "# plt.plot(feature1_values, predictions)\n",
    "# plt.xlabel(\"Feature 1 value\")\n",
    "# plt.ylabel(\"Predicted class\")\n",
    "# plt.title(\"Sensitivity analysis\")\n",
    "# plt.show()\n",
    "\n",
    "# # Perform scenario testing\n",
    "# # Scenario 1: Increase the value of feature2 by 20%\n",
    "# new_data['feature2'] *= 1.2\n",
    "# scenario1_predictions = best_dt_classifier.predict(new_data.drop('target', axis=1))\n",
    "# print(\"Scenario 1 predictions:\", scenario1_predictions)\n",
    "\n",
    "# # Scenario 2: Decrease the value of feature3 by 15%\n",
    "# new_data['feature3'] *= 0.85\n",
    "# scenario2_predictions = best_dt_classifier.predict(new_data.drop('target', axis=1))\n",
    "# print(\"Scenario 2 predictions:\", scenario2_predictions)\n",
    "# In this code, we:\n",
    "\n",
    "# Apply the decision tree model to new data and evaluate its performance.\n",
    "# Test the robustness of the model to changes in the dataset by removing 20% of the data and adding noise to the data.\n",
    "# Perform sensitivity analysis by varying the value of a feature and observing the change in predictions.\n",
    "# Perform scenario testing by creating hypothetical scenarios and observing the model's predictions.\n",
    "# By validating the decision tree model and exploring its uncertainty and risks, we can gain confidence in its performance and identify potential areas for improvement.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
