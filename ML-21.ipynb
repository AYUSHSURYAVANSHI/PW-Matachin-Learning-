{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is the mathematical formula for a linear SVM?\n",
    "# Answer :-\n",
    "# The mathematical formula for a linear Support Vector Machine (SVM) can be expressed as follows:\n",
    "\n",
    "# Given a set of training data points (x1,y1),(X2,y2),...,(xn,yn), where \n",
    "\n",
    "# xi represents the feature vector of the ith data point and \n",
    "# yi represents the class label (-1 or 1), the linear SVM aims to find a hyperplane that separates the data into two classes. The hyperplane is represented by the equation:\n",
    "\n",
    "\n",
    "# f(x)=w⋅x+b\n",
    "\n",
    "# Here:\n",
    "\n",
    "# w is the weight vector (coefficients) that defines the orientation of the hyperplane.\n",
    "# x is the input feature vector.\n",
    "# b is the bias term.\n",
    "# The decision function f(x) assigns a given input vector x to one of the two classes based on the sign of f(x)>0, the input is classified as class 1, and if f(x)<0, it is classified as class -1.\n",
    "# The goal of SVM training is to find the values of w and b that maximize the margin, which is the distance between the hyperplane and the nearest data point from either class. This is subject to the constraint that all data points are correctly classified, meaning yi⋅f(xi )≥1 for all training examples.\n",
    "\n",
    "# The optimization problem for linear SVM can be formulated as:\n",
    "\n",
    "# minimize(1/2∥w∥^2)\n",
    "\n",
    "# subject to  yi⋅(w⋅xi+b)≥1 for all i\n",
    "\n",
    "# This is a quadratic optimization problem, and various optimization algorithms, such as the Sequential Minimal Optimization (SMO) algorithm, are commonly used to find the optimal values of w and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What is the objective function of a linear SVM?\n",
    "# Answer :- \n",
    "# The objective function of a linear Support Vector Machine (SVM) is to maximize the margin between the two classes while correctly classifying the training data. In a binary classification problem, where the classes are labeled as -1 and 1, the objective function can be expressed as the minimization of:\n",
    "\n",
    "# 1/2∥w∥^2\n",
    " \n",
    "\n",
    "# subject to the constraint:\n",
    "\n",
    "# yi⋅(w⋅xi+b)≥1\n",
    "\n",
    "# Here:\n",
    "\n",
    "# w is the weight vector (coefficients) that defines the orientation of the hyperplane.xi is the feature vector of the ith data point.\n",
    "# b is the bias term.\n",
    "# yi is the class label of the ith data point.\n",
    "# The objective function 1/2∥w∥^2 represents the norm of the weight vector, and the goal is to minimize this norm. Minimizing the norm of w maximizes the margin between the two classes. The margin is the distance between the hyperplane and the nearest data point from either class.\n",
    "\n",
    "# The constraint yi⋅(w⋅xi +b)≥1 ensures that all data points are correctly classified, and each data point lies on the correct side of the decision boundary. The term yi⋅(w⋅xi+b) essentially measures the signed distance of a point xi from the decision hyperplane.\n",
    "# The objective is to find the values of w and b that satisfy the constraint and simultaneously minimize the norm of \n",
    "# w. This leads to finding a hyperplane that maximizes the margin while correctly separating the classes. The optimization problem is typically solved using techniques such as quadratic programming or other optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is the kernel trick in SVM?\n",
    "# Answer :-The kernel trick is a technique used in Support Vector Machines (SVMs) to implicitly map input data into a higher-dimensional space. SVMs are originally designed for linearly separable data, but many real-world datasets are not linearly separable in their original feature space. The kernel trick allows SVMs to effectively handle non-linear relationships between features by implicitly transforming the data into a higher-dimensional space.\n",
    "\n",
    "# The basic idea behind the kernel trick is to replace the dot product of the input vectors in the feature space with the evaluation of a kernel function. The dot product in the higher-dimensional space is computationally expensive, but the kernel function provides a way to compute it efficiently without explicitly transforming the data into that space.\n",
    "\n",
    "# The general form of the SVM decision function using the kernel trick is:\n",
    "\n",
    "\n",
    "# f(x)=∑i=1 N αi yi K(xi,x)+b\n",
    "\n",
    "# Here:\n",
    "\n",
    "# αi are the Lagrange multipliers obtained during the training of the SVM.\n",
    "# yi is the class label of the ith training example.\n",
    "\n",
    "# xi is the ith training example.\n",
    "# x is the input vector to be classified.\n",
    "\n",
    "# K(xi,x) is the kernel function that computes the dot product between \n",
    "# xi and x in the higher-dimensional space.\n",
    "# Commonly used kernel functions include:\n",
    "\n",
    "# Linear Kernel: \n",
    "# K(xi,x)=xi⋅x\n",
    "# Polynomial Kernel: \n",
    "# K(xi,x)=(xi⋅x+c)d\n",
    " \n",
    "# Radial Basis Function (RBF) or Gaussian Kernel: \n",
    "# K(xi,x)=e^−2σ^2∥xi−x∥^2\n",
    " \n",
    " \n",
    "# Sigmoid Kernel:K(xi,x)=tanh(αxi⋅x+c)\n",
    "# The choice of the kernel depends on the nature of the data and the problem at hand. The kernel trick allows SVMs to capture complex, non-linear decision boundaries without explicitly computing the transformation of the data into a higher-dimensional space, making it computationally efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What is the role of support vectors in SVM Explain with example.\n",
    "# Answer :-\n",
    "# In Support Vector Machines (SVM), support vectors play a crucial role in defining the decision boundary (hyperplane) that separates different classes. Support vectors are the data points that lie closest to the decision boundary, and they are the ones that have the most influence on the placement and orientation of the hyperplane. The term \"support vector\" is derived from the fact that these points support or define the position of the separating hyperplane.\n",
    "\n",
    "# Here's an explanation with an example:\n",
    "\n",
    "# Example:\n",
    "# Consider a binary classification problem with two classes, labeled as -1 and 1. The dataset consists of points in a two-dimensional space: \n",
    "\n",
    "# (x1,y1),(x2,y2),...,(xn,yn ), where \n",
    "# xi is the feature vector of the ith data point, and yi  is the class label.\n",
    "\n",
    "# The goal of SVM is to find a hyperplane represented by \n",
    "\n",
    "\n",
    "# f(x)=w⋅x+b that separates the data into two classes while maximizing the margin.\n",
    "\n",
    "# Training the SVM:\n",
    "\n",
    "# During the training process, the SVM identifies the support vectors, which are the data points that are closest to the decision boundary.\n",
    "# The decision boundary is defined by the equation \n",
    "\n",
    "# f(x)=0, and the support vectors satisfy \n",
    "\n",
    "# yi⋅f(xi )=1, where \n",
    "# yi is the class label of the ith support vector.\n",
    "# Defining the Decision Boundary:\n",
    "\n",
    "# The support vectors are critical because they determine the orientation of the hyperplane. The hyperplane is positioned so that it maximizes the margin between the two classes.\n",
    "# The margin is the perpendicular distance from the hyperplane to the nearest support vector. Maximizing this margin helps improve the generalization of the SVM to new, unseen data.\n",
    "# Testing/Classification:\n",
    "\n",
    "# When a new data point is presented to the trained SVM for classification, its position relative to the decision boundary (hyperplane) is determined.\n",
    "# The sign of \n",
    "\n",
    "# f(x) indicates the predicted class. If \n",
    "\n",
    "# f(x)>0, the point belongs to class 1; if \n",
    "\n",
    "# f(x)<0, it belongs to class -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "# SVM?\n",
    "# Answer :-1. Hyperplane:\n",
    "# The hyperplane is the decision boundary that separates different classes in SVM.\n",
    "\n",
    "# Example:\n",
    "# Consider a simple 2D dataset with two classes, red (positive) and blue (negative). The hyperplane is a line that separates the two classes.\n",
    "\n",
    "# Hyperplane\n",
    "\n",
    "# In this case, the line (hyperplane) is the decision boundary. Points on one side of the line are classified as red, and points on the other side are classified as blue.\n",
    "\n",
    "# 2. Marginal Plane:\n",
    "# The marginal plane is the plane that is equidistant from the hyperplane but on either side.\n",
    "\n",
    "# Example:\n",
    "# Let's extend the example above. The marginal planes are parallel to the hyperplane and define the margin.\n",
    "\n",
    "# Marginal Plane\n",
    "\n",
    "# The points on the marginal planes are considered support vectors and contribute to defining the margin. The distance between the hyperplane and the marginal plane is the margin.\n",
    "\n",
    "# 3. Hard Margin:\n",
    "# In a hard margin SVM, the goal is to find a hyperplane that perfectly separates the classes without any misclassifications.\n",
    "\n",
    "# Example:\n",
    "# In a perfectly separable dataset, a hard margin SVM finds a hyperplane with a maximum margin.\n",
    "\n",
    "# Hard Margin\n",
    "\n",
    "# In the example, the hyperplane separates the classes with the maximum margin, and all points are correctly classified.\n",
    "\n",
    "# 4. Soft Margin:\n",
    "# In a soft margin SVM, some misclassifications are allowed to find a hyperplane with a wider margin, making the model more tolerant to outliers.\n",
    "\n",
    "# Example:\n",
    "# Consider a dataset with outliers. A soft margin SVM allows for some misclassifications to achieve a wider margin.\n",
    "\n",
    "# Soft Margin\n",
    "\n",
    "# In this example, a few points are misclassified, but the margin is wider compared to the hard margin case, making the model more robust to noise.\n",
    "\n",
    "# These visualizations provide an overview of how the hyperplane, marginal plane, and margin are defined in SVM. The choice between hard and soft margins depends on the nature of the data and the level of tolerance to misclassifications. Soft margin SVMs are often preferred in real-world scenarios where the data may not be perfectly separable or contain outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. SVM Implementation through Iris dataset.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model.\n",
    "\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.\n",
    "Answer :-\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Using only the first two features for simplicity\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier\n",
    "def train_linear_svm(X_train, y_train, C_value=1.0):\n",
    "    svm_classifier = SVC(kernel='linear', C=C_value)\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    return svm_classifier\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "def predict_labels(classifier, X_test):\n",
    "    return classifier.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model on the testing set\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Plot the decision boundaries of the trained model using two features\n",
    "def plot_decision_boundaries(X, y, classifier, title):\n",
    "    h = .02  # Step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.3)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()\n",
    "\n",
    "# Train the model and make predictions\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "for C_value in C_values:\n",
    "    svm_classifier = train_linear_svm(X_train, y_train, C_value)\n",
    "    y_pred = predict_labels(svm_classifier, X_test)\n",
    "    \n",
    "    accuracy = compute_accuracy(y_test, y_pred)\n",
    "    print(f\"Accuracy with C={C_value}: {accuracy:.2f}\")\n",
    "\n",
    "    # Plot decision boundaries for two features\n",
    "    plot_decision_boundaries(X_train, y_train, svm_classifier, f\"Decision Boundaries (C={C_value})\")\n",
    "\n",
    "# Linear SVM from scratch\n",
    "class LinearSVM:\n",
    "    def __init__(self, learning_rate=0.01, num_epochs=1000, C=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.C = C\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.w = np.zeros(n)\n",
    "        self.b = 0\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            hinge_loss = 1 - y * (X.dot(self.w) + self.b)\n",
    "            hinge_loss[hinge_loss < 0] = 0\n",
    "            gradient_w = -1 / m * (X.T.dot(hinge_loss * y) + self.C * self.w)\n",
    "            gradient_b = -1 / m * np.sum(hinge_loss * y)\n",
    "            self.w -= self.learning_rate * gradient_w\n",
    "            self.b -= self.learning_rate * gradient_b\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(X.dot(self.w) + self.b)\n",
    "\n",
    "# Convert labels to -1 and 1 for SVM\n",
    "y_train_svm = 2 * y_train - 1\n",
    "y_test_svm = 2 * y_test - 1\n",
    "\n",
    "# Train linear SVM from scratch\n",
    "svm_from_scratch = LinearSVM(C=1.0)\n",
    "svm_from_scratch.fit(X_train, y_train_svm)\n",
    "\n",
    "# Predict labels\n",
    "y_pred_svm_from_scratch = svm_from_scratch.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy_from_scratch = compute_accuracy(y_test_svm, y_pred_svm_from_scratch)\n",
    "print(f\"Accuracy of SVM from scratch: {accuracy_from_scratch:.2f}\")\n",
    "\n",
    "# Plot decision boundaries for two features\n",
    "plot_decision_boundaries(X_train, y_train_svm, svm_from_scratch, \"Decision Boundaries (SVM from scratch)\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
