{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffrc btw Objct Dtctio ad Objct Classificatio.\n",
    "# a. Explain the difference between object detection and object classification in the\n",
    "# context of computer vision tasks. Provide examples to illustrate each concept.\n",
    "# Answer :\n",
    "# The difference between object detection and object classification lies in their goals and outputs.\n",
    "\n",
    "# Object Classification: Object classification, also known as image classification, involves assigning a class label to an entire image. The goal is to predict the type or class of an object present in the image. For example, in a self-driving car application, the system might classify an image as \"car,\" \"pedestrian,\" or \"road sign.\"\n",
    "\n",
    "# Example: Suppose we have an image of a dog. The object classification model would output a class label, such as \"dog.\"\n",
    "\n",
    "# Object Detection: Object detection involves locating and classifying objects within an image. The goal is to identify the location and type of objects present in the image. Object detection models output a bounding box around each object, along with a class label.\n",
    "\n",
    "# Example: Suppose we have an image of a street scene with multiple objects, such as cars, pedestrians, and road signs. The object detection model would output a set of bounding boxes, each containing an object, along with a class label, such as \"car,\" \"pedestrian,\" or \"road sign.\"\n",
    "\n",
    "# To illustrate the difference, consider the following examples:\n",
    "\n",
    "# Image Classification:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # Load the image\n",
    "# img = tf.io.read_file('image.jpg')\n",
    "\n",
    "# # Preprocess the image\n",
    "# img = tf.image.resize(img, (224, 224))\n",
    "# img = img / 255.0\n",
    "\n",
    "# # Create a classification model\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "#     tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(128, activation='relu'),\n",
    "#     tf.keras.layers.Dense(10, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = model.predict(img)\n",
    "# Object Detection:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # Load the image\n",
    "# img = tf.io.read_file('image.jpg')\n",
    "\n",
    "# # Preprocess the image\n",
    "# img = tf.image.resize(img, (416, 416))\n",
    "# img = img / 255.0\n",
    "\n",
    "# # Create an object detection model (e.g., YOLOv3)\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(416, 416, 3)),\n",
    "#     tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "#     tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "#     tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(128, activation='relu'),\n",
    "#     tf.keras.layers.Dense(4, activation='sigmoid')  # Output: x, y, w, h\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = model.predict(img)\n",
    "# In the object detection example, the model outputs a set of bounding boxes, each containing an object, along with a class label. The output shape would be (num_objects, 4 + num_classes), where num_objects is the number of detected objects, and num_classes is the number of classes.\n",
    "\n",
    "# In summary, object classification involves assigning a class label to an entire image, while object detection involves locating and classifying objects within an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Scarios whr Objct Dtctio is usd:\n",
    "# a. Describe at least three scenarios or real-world applications where object detection\n",
    "# techniques are commonly used. Explain the significance of object detection in these scenarios\n",
    "# and how it benefits the respective applications.\n",
    "# Answer :\n",
    "# Scenario 1: Self-Driving Cars\n",
    "\n",
    "# Object detection plays a crucial role in self-driving cars, as it enables the vehicle to detect and respond to various objects on the road, such as pedestrians, other cars, traffic lights, and road signs. The significance of object detection in this scenario lies in its ability to ensure safety and prevent accidents.\n",
    "\n",
    "# Object detection algorithms, such as YOLO (You Only Look Once) or SSD (Single Shot Detector), are used to analyze images from cameras and other sensors mounted on the vehicle. These algorithms can detect objects in real-time, allowing the vehicle to react accordingly. For example, if a pedestrian is detected stepping into the road, the vehicle can automatically apply the brakes to avoid a collision.\n",
    "\n",
    "# The benefits of object detection in self-driving cars include:\n",
    "\n",
    "# Improved safety: Object detection helps prevent accidents by detecting potential hazards and responding accordingly.\n",
    "# Enhanced navigation: Object detection enables the vehicle to navigate through complex environments, such as construction zones or intersections with multiple lanes.\n",
    "# Increased efficiency: Object detection can optimize the vehicle's route and speed, reducing travel time and improving fuel efficiency.\n",
    "# Scenario 2: Surveillance Systems\n",
    "\n",
    "# Object detection is widely used in surveillance systems to detect and track people, vehicles, and other objects in real-time. The significance of object detection in this scenario lies in its ability to enhance security and prevent criminal activity.\n",
    "\n",
    "# Object detection algorithms are used to analyze video feeds from cameras installed in public areas, such as shopping malls, airports, and city streets. These algorithms can detect and track objects, such as people or vehicles, and alert security personnel to potential threats.\n",
    "\n",
    "# The benefits of object detection in surveillance systems include:\n",
    "\n",
    "# Improved security: Object detection helps prevent criminal activity by detecting and tracking suspicious objects or individuals.\n",
    "# Enhanced situational awareness: Object detection provides security personnel with real-time information about the environment, enabling them to respond quickly to potential threats.\n",
    "# Increased efficiency: Object detection can automate the monitoring process, reducing the need for human operators to constantly monitor video feeds.\n",
    "# Scenario 3: Medical Imaging\n",
    "\n",
    "# Object detection is used in medical imaging to detect and diagnose diseases, such as cancer, from medical images like X-rays, CT scans, and MRI scans. The significance of object detection in this scenario lies in its ability to improve diagnostic accuracy and patient outcomes.\n",
    "\n",
    "# Object detection algorithms are used to analyze medical images and detect abnormalities, such as tumors or fractures. These algorithms can also help doctors identify the location and size of the abnormality, enabling them to develop effective treatment plans.\n",
    "\n",
    "# The benefits of object detection in medical imaging include:\n",
    "\n",
    "# Improved diagnostic accuracy: Object detection helps doctors detect diseases at an early stage, improving treatment outcomes and patient survival rates.\n",
    "# Enhanced patient care: Object detection enables doctors to develop personalized treatment plans, improving patient care and reducing healthcare costs.\n",
    "# Increased efficiency: Object detection can automate the image analysis process, reducing the time and effort required to diagnose diseases.\n",
    "# In each of these scenarios, object detection plays a critical role in improving safety, security, and efficiency. By detecting and responding to objects in real-time, object detection algorithms can help prevent accidents, enhance security, and improve diagnostic accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Imag Data as Structurd Data:\n",
    "# a. Discuss whether image data can be considered a structured form of data. Provide reasoning\n",
    "# and examples to support your answer.\n",
    "# Answer :\n",
    "\n",
    "# Image Data as Structured Data\n",
    "\n",
    "# Image data can be considered a semi-structured form of data, rather than a fully structured form of data. Here's why:\n",
    "\n",
    "# Reasoning\n",
    "\n",
    "# Structured data refers to data that is highly organized, easily searchable, and machine-readable. Examples of structured data include relational databases, CSV files, and Excel spreadsheets. In contrast, image data is a type of unstructured data, which means it does not conform to a predefined format or organization.\n",
    "\n",
    "# However, image data can be considered semi-structured because it often contains inherent structures, such as:\n",
    "\n",
    "# Pixel grid: Images are composed of a grid of pixels, which can be thought of as a two-dimensional array of values.\n",
    "# Color channels: Images typically have three color channels (red, green, and blue), which can be represented as separate arrays of values.\n",
    "# Metadata: Images often contain metadata, such as EXIF data, which includes information like camera settings, timestamp, and geolocation.\n",
    "# Examples\n",
    "\n",
    "# Image compression: Image compression algorithms, like JPEG, take advantage of the structured nature of image data to reduce the file size. They exploit the redundancy in the pixel grid and color channels to represent the image more efficiently.\n",
    "# Object detection: Object detection algorithms, like YOLO, use the structured nature of image data to identify objects within an image. They analyze the pixel grid and color channels to detect patterns and shapes that correspond to specific objects.\n",
    "# Image segmentation: Image segmentation algorithms, like thresholding, use the structured nature of image data to separate objects from the background. They analyze the pixel grid and color channels to identify regions of similar intensity or color.\n",
    "# Counterarguments\n",
    "\n",
    "# While image data has some structured elements, it is still largely unstructured. Here are some counterarguments:\n",
    "\n",
    "# Lack of explicit schema: Unlike structured data, image data does not have an explicit schema or format that defines its organization.\n",
    "# Variability in format: Image data can come in various formats, such as JPEG, PNG, GIF, and TIFF, each with its own set of characteristics and compression algorithms.\n",
    "# High dimensionality: Image data is often high-dimensional, making it difficult to analyze and process using traditional structured data techniques.\n",
    "# Conclusion\n",
    "\n",
    "# In conclusion, while image data has some structured elements, it is not fully structured data. It can be considered semi-structured due to its inherent structures, such as the pixel grid and color channels. However, its lack of explicit schema, variability in format, and high dimensionality make it distinct from traditional structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Explaiig Iformatio i a Imag for CNN:\n",
    "# a. Explain how Convolutional Neural Networks (CNN) can extract and understand information\n",
    "# from an image. Discuss the key components and processes involved in analyzing image data\n",
    "# using CNNs.\n",
    "# Answer :\n",
    "# Explaiig Iformatio i a Imag for CNN: a. Explain how Convolutional Neural Networks (CNN) can extract and understand information from an image. Discuss the key components and processes involved in analyzing image data using CNNs.\n",
    "# A Convolutional Neural Network (CNN) extracts and understands information from an image by using a hierarchical representation to identify patterns and features. The key components involved in this process are:\n",
    "\n",
    "# Convolutional Layers: These layers apply filters to small regions of the image to detect features such as edges, textures, and shapes. The output of these layers is a feature map that highlights the presence of these features.\n",
    "# Pooling Layers: These layers downsample the feature maps to reduce the spatial dimensions and retain the most important information. This process helps to reduce the number of parameters and the computational complexity of the network.\n",
    "# Fully Connected Layers: These layers are used to make predictions based on the high-level features learned by the previous layers. They connect every neuron in one layer to every neuron in the next layer.\n",
    "# The process of analyzing image data using CNNs involves the following steps:\n",
    "\n",
    "# Data Preprocessing: The input images are preprocessed to ensure they are in the same format and size.\n",
    "# Convolutional Operations: The convolutional layers apply filters to the input image to detect features.\n",
    "# Pooling: The pooling layers downsample the feature maps to reduce the spatial dimensions.\n",
    "# Flattening: The output of the convolutional and pooling layers is flattened into a one-dimensional feature vector.\n",
    "# Fully Connected Layers: The fully connected layers make predictions based on the high-level features learned by the previous layers.\n",
    "# The key processes involved in analyzing image data using CNNs are:\n",
    "\n",
    "# Feature Extraction: The convolutional and pooling layers extract features from the input image.\n",
    "# Feature Representation: The fully connected layers represent the features in a hierarchical manner.\n",
    "# Prediction: The output layer makes predictions based on the feature representation.\n",
    "# Here is an example of a simple CNN architecture in Python using Keras:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# This architecture consists of multiple convolutional and pooling layers to extract features from the input image, followed by fully connected layers to make predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q Flattig Imags for ANN:\n",
    "# a. Discuss why it is not recommended to flatten images directly and input them into an\n",
    "# Artificial Neural Network (ANN) for image classification. Highlight the limitations and\n",
    "# challenges associated with this approach.\n",
    "# Answer :\n",
    "# Flattening Images for ANN: Limitations and Challenges\n",
    "\n",
    "# Flattening images directly and inputting them into an Artificial Neural Network (ANN) for image classification is not a recommended approach. This method involves converting a 2D image into a 1D array, which can lead to several limitations and challenges.\n",
    "\n",
    "# Limitations:\n",
    "\n",
    "# Loss of Spatial Information: When an image is flattened, the spatial relationships between pixels are lost. This is a critical issue in image classification, as the spatial arrangement of features is essential for recognizing objects and patterns.\n",
    "# Increased Dimensionality: Flattening an image results in a high-dimensional feature vector, which can lead to the curse of dimensionality. This makes it challenging for the ANN to learn meaningful patterns and relationships between features.\n",
    "# Computational Complexity: Processing high-dimensional feature vectors can be computationally expensive, leading to increased training times and reduced model performance.\n",
    "# Challenges:\n",
    "\n",
    "# Feature Extraction: ANNs are not designed to extract features from raw pixel values. They require pre-processed features that are relevant to the classification task. Flattening images does not provide a meaningful way to extract features.\n",
    "# Overfitting: With a high-dimensional feature space, the ANN may overfit the training data, leading to poor generalization performance on unseen data.\n",
    "# Lack of Translation Invariance: Flattening images does not preserve translation invariance, which is essential in image classification. This means that the ANN may not be able to recognize objects or patterns when they are translated or rotated.\n",
    "# Alternative Approaches:\n",
    "\n",
    "# Convolutional Neural Networks (CNNs): CNNs are specifically designed to handle image data and can extract features from raw pixel values. They are translation invariant and can learn spatial hierarchies of features.\n",
    "# Data Augmentation: Data augmentation techniques, such as rotation, flipping, and cropping, can be used to increase the size of the training dataset and improve the robustness of the ANN.\n",
    "# Feature Extraction Techniques: Techniques like edge detection, texture analysis, and shape extraction can be used to extract meaningful features from images, which can then be input into an ANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applig CNN to th MNIST Datast:\n",
    "# a. Explain why it is not necessary to apply CNN to the MNIST dataset for image classification.\n",
    "# Discuss the characteristics of the MNIST dataset and how it aligns with the requirements of\n",
    "# CNNs.\n",
    "# Answer :\n",
    "# Applig CNN to th MNIST Datast: a. Explain why it is not necessary to apply CNN to the MNIST dataset for image classification. Discuss the characteristics of the MNIST dataset and how it aligns with the requirements of CNNs.\n",
    "\n",
    "# The MNIST dataset consists of 28x28 grayscale images of handwritten digits (0-9), with a training set of 60,000 examples and a test set of 10,000 examples. While Convolutional Neural Networks (CNNs) are powerful models for image classification tasks, they might not be the best choice for the MNIST dataset.\n",
    "\n",
    "# One reason is that the MNIST dataset comprises relatively small and simple images. The images are already normalized to a fixed size (28x28), and the features are not complex or varied. CNNs are typically useful when dealing with larger images or more complex features, where the convolutional and pooling layers can effectively extract and downsample the features.\n",
    "\n",
    "# Another reason is that the MNIST dataset can be easily classified using simpler models, such as Multilayer Perceptrons (MLPs) or even traditional machine learning techniques like Support Vector Machines (SVMs) or Random Forests. The dataset is relatively small, and the classes are well-separated, making it easier to achieve high accuracy with simpler models.\n",
    "\n",
    "# Here is an example of a simple MLP model in Python using Keras:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(64, activation='relu', input_shape=(784,)))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# In contrast, CNNs are more complex models that require a significant amount of computational resources and data to train effectively. While they can be used for the MNIST dataset, they might not provide a significant improvement in accuracy compared to simpler models.\n",
    "\n",
    "# In summary, while CNNs are powerful models for image classification, they might not be the best choice for the MNIST dataset due to its simplicity and small size. Simpler models like MLPs or traditional machine learning techniques can achieve high accuracy with less computational resources and complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extractig Faturs at Local Spac:\n",
    "# a. Justify why it is important to extract features from an image at the local level rather than\n",
    "# considering the entire image as a whole. Discuss the advantages and insights gained by\n",
    "# performing local feature extraction.\n",
    "# Answer :\n",
    "# Extracting Features at Local Space\n",
    "# Why Local Feature Extraction is Important\n",
    "# Extracting features from an image at the local level is crucial rather than considering the entire image as a whole. This approach is important for several reasons:\n",
    "\n",
    "# 1. Robustness to Variations\n",
    "# Local feature extraction is more robust to variations in lighting, pose, and viewpoint. When considering the entire image, these variations can significantly affect the feature extraction process. By focusing on local regions, the impact of these variations is minimized.\n",
    "\n",
    "# 2. Capturing Local Patterns\n",
    "# Local features are better suited to capture local patterns and textures that are essential for object recognition and image understanding. These patterns may be lost when considering the entire image as a whole.\n",
    "\n",
    "# 3. Reducing Dimensionality\n",
    "# Local feature extraction reduces the dimensionality of the feature space, making it more manageable and efficient for processing and analysis.\n",
    "\n",
    "# 4. Improved Object Recognition\n",
    "# Local features are more effective in object recognition tasks, as they can capture the distinctive characteristics of an object, such as edges, corners, and shapes.\n",
    "\n",
    "# Advantages and Insights Gained\n",
    "# Performing local feature extraction offers several advantages and insights, including:\n",
    "\n",
    "# 1. Improved Accuracy\n",
    "# Local feature extraction leads to improved accuracy in object recognition and image classification tasks.\n",
    "\n",
    "# 2. Increased Robustness\n",
    "# Local features are more robust to noise, occlusion, and other forms of image degradation.\n",
    "\n",
    "# 3. Better Representation\n",
    "# Local features provide a better representation of the image, capturing the essential characteristics of the object or scene.\n",
    "\n",
    "# 4. Efficient Processing\n",
    "# Local feature extraction is more efficient in terms of processing time and computational resources.\n",
    "\n",
    "# In conclusion, extracting features from an image at the local level is essential for robust and accurate object recognition, image understanding, and classification tasks. The advantages and insights gained from local feature extraction make it a crucial step in many computer vision applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ì§ Importac of Covolutio ad Max Poolig:\n",
    "# a. Elaborate on the importance of convolution and max pooling operations in a Convolutional\n",
    "# Neural Network (CNN). Explain how these operations contribute to feature extraction and\n",
    "# spatial down-sampling in CNNs.\n",
    "# Answer :\n",
    "# Convolutional Neural Networks (CNNs)\n",
    "# Convolutional Neural Networks (CNNs) are a type of neural network architecture that have revolutionized the field of computer vision. Two essential operations in CNNs are convolution and max pooling, which play a crucial role in feature extraction and spatial down-sampling.\n",
    "\n",
    "# Convolution Operation\n",
    "# The convolution operation is a fundamental component of CNNs. It involves scanning an image with a set of learnable filters, which are small matrices that slide over the entire image, performing a dot product at each position to generate a feature map. The convolution operation is essential for feature extraction in CNNs, as it:\n",
    "\n",
    "# 1. Detects Local Patterns\n",
    "# Convolutional filters detect local patterns in the image, such as edges, lines, and textures, which are essential for object recognition.\n",
    "\n",
    "# 2. Captures Spatial Hierarchies\n",
    "# The convolution operation captures spatial hierarchies in the image, allowing the network to recognize objects at multiple scales.\n",
    "\n",
    "# 3. Reduces Spatial Dimensions\n",
    "# Convolution reduces the spatial dimensions of the feature maps, allowing the network to focus on the most important features.\n",
    "\n",
    "# Max Pooling Operation\n",
    "# Max pooling is a down-sampling operation that follows the convolution operation. It involves dividing the feature map into rectangular regions and selecting the maximum value from each region. Max pooling is essential for spatial down-sampling in CNNs, as it:\n",
    "\n",
    "# 1. Reduces Spatial Dimensions\n",
    "# Max pooling reduces the spatial dimensions of the feature maps, reducing the number of parameters and computations required.\n",
    "\n",
    "# 2. Increases Translation Invariance\n",
    "# Max pooling increases translation invariance, allowing the network to recognize objects regardless of their position in the image.\n",
    "\n",
    "# 3. Improves Robustness\n",
    "# Max pooling improves the robustness of the network to small transformations, such as rotations and flips.\n",
    "\n",
    "# Contribution to Feature Extraction and Spatial Down-Sampling\n",
    "# The convolution and max pooling operations contribute significantly to feature extraction and spatial down-sampling in CNNs. The convolution operation extracts local features, while max pooling reduces the spatial dimensions of the feature maps, allowing the network to focus on the most important features.\n",
    "\n",
    "# The combination of convolution and max pooling operations enables CNNs to:\n",
    "\n",
    "# 1. Extract Hierarchical Features\n",
    "# Extract hierarchical features, which are essential for object recognition and image classification.\n",
    "\n",
    "# 2. Reduce Spatial Dimensions\n",
    "# Reduce spatial dimensions, allowing the network to process images efficiently.\n",
    "\n",
    "# 3. Improve Robustness\n",
    "# Improve robustness to small transformations, such as rotations and flips.\n",
    "\n",
    "# In conclusion, the convolution and max pooling operations are essential components of CNNs, contributing significantly to feature extraction and spatial down-sampling. These operations enable CNNs to recognize objects accurately and efficiently, making them a crucial part of many computer vision applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
