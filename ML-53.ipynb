{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q1. Install and load the latest versions of TensorFlow and Keras. Print their versions.\n",
    "# # Answer :\n",
    "# Here's how you can install and load the latest versions of TensorFlow and Keras, and print their versions:\n",
    "\n",
    "# Install TensorFlow and Keras:\n",
    "\n",
    "# You can install TensorFlow and Keras using pip, which is the Python package installer. Here are the commands:\n",
    "\n",
    "\n",
    "# pip install tensorflow\n",
    "# pip install keras\n",
    "# Load TensorFlow and Keras:\n",
    "\n",
    "# After installation, you can load TensorFlow and Keras in your Python script or code:\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# Print Versions:\n",
    "\n",
    "# To print the versions of TensorFlow and Keras, you can use the following code:\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "# print(\"TensorFlow Version:\", tf.__version__)\n",
    "# print(\"Keras Version:\", keras.__version__)\n",
    "# When you run this code, it will print the versions of TensorFlow and Keras that you have installed.\n",
    "\n",
    "# Output:\n",
    "\n",
    "# Here's an example output:\n",
    "\n",
    "# TensorFlow Version: 2.7.0\n",
    "# Keras Version: 2.7.0\n",
    "# Note that the versions may vary depending on the latest versions available at the time of installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q2. Load the Wine Quality dataset and explore its dimensions.\n",
    "# # Answer :\n",
    "# The Wine Quality dataset is a popular dataset used in machine learning and data analysis. It contains 1599 samples of red wine, each with 11 laboratory measurements and a quality rating. The dataset is divided into 12 variables:\n",
    "\n",
    "# Fixed Acidity\n",
    "# Volatile Acidity\n",
    "# Citric Acid\n",
    "# Residual Sugar\n",
    "# Chlorides\n",
    "# Free Sulfur Dioxide\n",
    "# Total Sulfur Dioxide\n",
    "# Density\n",
    "# pH\n",
    "# Sulphates\n",
    "# Alcohol\n",
    "# Quality (rated on a scale from 0 to 10)\n",
    "# The dataset has 1599 observations (rows) and 12 variables (columns). The quality rating is the target variable, and the other 11 variables are the features used to predict the quality of the wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q3. Check for null values, identify categorical variables, and encode them.\n",
    "# # Answer :\n",
    "# Check for null values: You can use the isnull().sum() function to check for null values in your dataset.\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # assume 'df' is your dataframe\n",
    "# print(df.isnull().sum())\n",
    "# Identify categorical variables: You can use the select_dtypes function to identify categorical variables.\n",
    "\n",
    "\n",
    "# categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "# print(categorical_cols)\n",
    "# Encode categorical variables: You can use the get_dummies function to encode categorical variables.\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # assume 'df' is your dataframe and 'categorical_cols' are your categorical columns\n",
    "# df_encoded = pd.get_dummies(df, columns=categorical_cols)\n",
    "# print(df_encoded.head())\n",
    "# Alternatively, you can use label encoding using the LabelEncoder class from scikit-learn.\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# for col in categorical_cols:\n",
    "#     df[col] = le.fit_transform(df[col])\n",
    "# print(df.head())\n",
    "# In Splunk, you can use the eval command to encode categorical variables.\n",
    "\n",
    "# splunk\n",
    "\n",
    "# | eval {fieldToEncode} = 1\n",
    "# | fillnull\n",
    "# Note that the above Splunk code is a manual way of encoding categorical variables, and it may not be as efficient as using the get_dummies function or LabelEncoder class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q4. Separate the features and target variables from the dataset.\n",
    "# # Answer :\n",
    "# To check for null values, identify categorical variables, and encode them, you can follow these steps:\n",
    "\n",
    "# Check for null values: You can use the isnull().sum() function to check for null values in your dataset.\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # assume 'df' is your dataframe\n",
    "# print(df.isnull().sum())\n",
    "# Identify categorical variables: You can use the select_dtypes function to identify categorical variables.\n",
    "\n",
    "# categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "# print(categorical_cols)\n",
    "# Encode categorical variables: You can use the get_dummies function to encode categorical variables.\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # assume 'df' is your dataframe and 'categorical_cols' are your categorical columns\n",
    "# df_encoded = pd.get_dummies(df, columns=categorical_cols)\n",
    "# print(df_encoded.head())\n",
    "# Alternatively, you can use label encoding using the LabelEncoder class from scikit-learn.\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# for col in categorical_cols:\n",
    "#     df[col] = le.fit_transform(df[col])\n",
    "# print(df.head())\n",
    "# In Splunk, you can use the eval command to encode categorical variables.\n",
    "\n",
    "# splunk\n",
    "\n",
    "# | eval {fieldToEncode} = 1\n",
    "# | fillnull\n",
    "# Note that the above Splunk code is a manual way of encoding categorical variables, and it may not be as efficient as using the get_dummies function or LabelEncoder class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Perform a train-test split, dividing the data into training, validation, and test datasets.\n",
    "# Answer :\n",
    "# To perform a train-test split, dividing the data into training, validation, and test datasets, you can use the train_test_split function from Scikit-learn's model_selection module. Here's an example:\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Assuming X is your feature data and y is your target variable\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42)\n",
    "# In this example, we first split the data into training and validation sets with a test size of 0.2 (i.e., 20% of the data will be used for validation). Then, we split the validation set into validation and test sets with a test size of 0.5 (i.e., 50% of the remaining data will be used for testing).\n",
    "\n",
    "# This will give you three datasets:\n",
    "\n",
    "# X_train and y_train for training the model\n",
    "# X_val and y_val for validating the model during hyperparameter tuning\n",
    "# X_test and y_test for evaluating the final performance of the model\n",
    "# Note that you can adjust the test_size parameter to control the proportion of data used for each set. Also, make sure to set the random_state parameter to ensure reproducibility of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Scale the dataset using an appropriate scaling technique.\n",
    "# Answer :\n",
    "# To scale the dataset, we can use the StandardScaler from scikit-learn. Here's how you can do it:\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Create a StandardScaler object\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# # Fit the scaler to the training data and transform both the training and test data\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "# In this code, we first create a StandardScaler object. Then, we fit the scaler to the training data using the fit_transform method, which calculates the mean and standard deviation of the training data and scales it accordingly. We also transform the test data using the transform method, which scales the test data using the same mean and standard deviation as the training data.\n",
    "\n",
    "# By scaling the dataset, we can prevent features with large ranges from dominating the model, and improve the performance of the model.\n",
    "\n",
    "# Alternatively, you can use other scaling techniques such as MinMaxScaler or Normalizer depending on the specific requirements of your dataset.\n",
    "\n",
    "# Here's an example of using MinMaxScaler:\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Create a MinMaxScaler object\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# # Fit the scaler to the training data and transform both the training and test data\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "# And here's an example of using Normalizer:\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# # Create a Normalizer object\n",
    "# scaler = Normalizer()\n",
    "\n",
    "# # Fit the scaler to the training data and transform both the training and test data\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "# Note that the choice of scaling technique depends on the specific characteristics of your dataset and the requirements of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Design and implement at least two hidden layers and an output layer for the binary categorical\n",
    "# variables.\n",
    "# Answer :\n",
    "# Here's an example implementation of a neural network with at least two hidden layers and an output layer for binary categorical variables using Python and the Keras library:\n",
    "\n",
    "# # Import necessary libraries\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# # Define the neural network model\n",
    "# model = Sequential()\n",
    "\n",
    "# # Add the first hidden layer with 64 neurons and ReLU activation\n",
    "# model.add(Dense(64, activation='relu', input_dim=10))  # 10 input features\n",
    "\n",
    "# # Add the second hidden layer with 32 neurons and ReLU activation\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# # Add the output layer with 1 neuron and sigmoid activation for binary classification\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # Compile the model with binary cross-entropy loss and Adam optimizer\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# In this example, we define a neural network with:\n",
    "\n",
    "# The input layer has 10 features.\n",
    "# The first hidden layer has 64 neurons with ReLU (Rectified Linear Unit) activation.\n",
    "# The second hidden layer has 32 neurons with ReLU activation.\n",
    "# The output layer has 1 neuron with sigmoid activation for binary classification.\n",
    "# The model is compiled with binary cross-entropy loss and the Adam optimizer.\n",
    "# You can adjust the number of neurons, layers, and activation functions based on your specific problem and dataset.\n",
    "\n",
    "# Note: This is just one example implementation, and you may need to experiment with different architectures and hyperparameters to achieve the best results for your specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. Create a Sequential model in Keras and add the previously designed layers to it.\n",
    "# Answer :\n",
    "# Here's how you can create a Sequential model in Keras and add the previously designed layers to it:\n",
    "\n",
    "# # Create a Sequential model\n",
    "# model = tf.keras.models.Sequential()\n",
    "\n",
    "# # Add the convolutional layer\n",
    "# model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "\n",
    "# # Add the max pooling layer\n",
    "# model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# # Add the flattening layer\n",
    "# model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# # Add the dense layer\n",
    "# model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "# # Add the dropout layer\n",
    "# model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "# # Add the final dense layer\n",
    "# model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "# In this code, we first create a Sequential model using tf.keras.models.Sequential(). Then, we add each of the previously designed layers to the model using the add() method.\n",
    "\n",
    "# Note that we need to specify the input shape for the first layer (the convolutional layer) using the input_shape parameter. This is because the model doesn't know the shape of the input data until we specify it.\n",
    "\n",
    "# Also, we use the activation parameter to specify the activation function for each layer. For example, we use the ReLU activation function for the convolutional and dense layers, and the softmax activation function for the final dense layer.\n",
    "\n",
    "# Once we've added all the layers, we can use the model for training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. Print the summary of the model architecture.\n",
    "# Answer :\n",
    "# To print the summary of the model architecture, you can use the summary() method in Keras. Here's an example:\n",
    "\n",
    "# # Print the summary of the model architecture\n",
    "# model.summary()\n",
    "# This will print a detailed summary of the model architecture, including the number of layers, the number of parameters, and the input and output shapes of each layer.\n",
    "\n",
    "# Here's an example output:\n",
    "\n",
    "# _________________________________________________________________\n",
    "# Layer (type)                 Output Shape              Param #   \n",
    "# =================================================================\n",
    "# input_1 (InputLayer)         (None, 28, 28, 1)        0         \n",
    "# _________________________________________________________________\n",
    "# conv2d (Conv2D)             (None, 26, 26, 32)       320       \n",
    "# _________________________________________________________________\n",
    "# max_pooling2d (MaxPooling2D) (None, 13, 13, 32)      0         \n",
    "# _________________________________________________________________\n",
    "# flatten (Flatten)           (None, 5408)            0         \n",
    "# _________________________________________________________________\n",
    "# dense (Dense)               (None, 128)             692224   \n",
    "# _________________________________________________________________\n",
    "# dropout (Dropout)           (None, 128)             0         \n",
    "# _________________________________________________________________\n",
    "# dense_1 (Dense)             (None, 10)              1290     \n",
    "# =================================================================\n",
    "# Total params: 693,834\n",
    "# Trainable params: 693,834\n",
    "# Non-trainable params: 0\n",
    "# _______________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10. Set the loss function(‘binary_crossentropy’), optimizer, and include the accuracy metric in the model.\n",
    "# Answer :\n",
    "\n",
    "# model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "# This code sets the loss function to binary cross-entropy, the optimizer to stochastic gradient descent (SGD), and includes the accuracy metric in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q11. Compile the model with the specified loss function, optimizer, and metrics.\n",
    "# Answer :\n",
    "# To compile the model, you need to specify the loss function, optimizer, and metrics. Here's an example:\n",
    "\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='categorical_crossentropy', \n",
    "#               optimizer='adam', \n",
    "#               metrics=['accuracy'])\n",
    "# In this example, we're compiling the model with:\n",
    "\n",
    "# loss='categorical_crossentropy': This specifies the loss function to use for training. In this case, we're using categorical cross-entropy, which is suitable for multi-class classification problems.\n",
    "# optimizer='adam': This specifies the optimizer to use for training. In this case, we're using the Adam optimizer, which is a popular choice for many deep learning tasks.\n",
    "# metrics=['accuracy']: This specifies the metrics to use for evaluating the model's performance. In this case, we're using accuracy, which measures the proportion of correctly classified samples.\n",
    "# By compiling the model with these specifications, we're telling Keras to use these settings for training and evaluation.\n",
    "\n",
    "# Note that you can customize the loss function, optimizer, and metrics to suit your specific problem and needs. For example, you might use loss='mean_squared_error' for regression problems, or optimizer='sgd' for stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q12. Fit the model to the training data using appropriate batch size and number of epochs.\n",
    "# Answer :\n",
    "# To fit the model to the training data, we need to specify the batch size and number of epochs. Let's assume we want to use a batch size of 128 and train the model for 10 epochs.\n",
    "\n",
    "# # Fit the model to the training data\n",
    "# history = model.fit(train_images, y_train, \n",
    "#                     batch_size=128, \n",
    "#                     epochs=10, \n",
    "#                     validation_data=(test_images, y_test), \n",
    "#                     verbose=2)\n",
    "# In this code, we're using the fit method to train the model on the training data train_images and y_train. We're also specifying the batch size as 128 and the number of epochs as 10. Additionally, we're passing the validation data test_images and y_test to monitor the model's performance on the validation set during training. The verbose parameter is set to 2 to display a progress bar during training. The history object will store the training history, which we can use to plot the accuracy and loss curves later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q13. Obtain the model's parameters (weights and biases).\n",
    "# Answer :\n",
    "# To obtain the model's parameters (weights and biases), you can use the get_weights() method in Keras. Here's an example:\n",
    "\n",
    "# # Get the model's weights and biases\n",
    "# weights = model.get_weights()\n",
    "\n",
    "# # Print the weights and biases\n",
    "# for i, weight in enumerate(weights):\n",
    "#     print(f\"Layer {i}: {weight.shape}\")\n",
    "#     print(weight)\n",
    "# This will print the weights and biases for each layer in the model, along with their shapes.\n",
    "\n",
    "# Alternatively, you can use the model.layers attribute to access the individual layers and their weights and biases. For example:\n",
    "\n",
    "# # Get the first layer's weights and biases\n",
    "# layer = model.layers[0]\n",
    "# weights = layer.get_weights()\n",
    "# print(weights)\n",
    "# This will print the weights and biases for the first layer in the model.\n",
    "\n",
    "# Note that the get_weights() method returns a list of NumPy arrays, where each array represents the weights and biases for a particular layer.\n",
    "\n",
    "# You can also use the model.summary() method to get a summary of the model's architecture, including the number of parameters (weights and biases) in each layer. For example:\n",
    "\n",
    "# model.summary()\n",
    "# This will print a summary of the model's architecture, including the number of parameters in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q14. Store the model's training history as a Pandas DataFrame.\n",
    "# Answer :\n",
    "# python\n",
    "# history = model.fit(X_train, y_train, epochs=10, \n",
    "#                     validation_data=(X_val, y_val), \n",
    "#                     verbose=0)\n",
    "\n",
    "# history_df = pd.DataFrame(history.history)\n",
    "# print(history_df.head())\n",
    "# This code snippet stores the model's training history as a Pandas DataFrame, which can be used for further analysis and visualization. The history.history attribute contains a dictionary of the training and validation metrics, which is converted to a DataFrame using pd.DataFrame. The head() method is used to print the first few rows of the resulting DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q15. Plot the training history (e.g., accuracy and loss) using suitable visualization techniques.\n",
    "# # Answer :\n",
    "# To plot the training history, you can use the matplotlib library in Python. Here's an example:\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming you have a history object from training a Keras model\n",
    "# history = model.fit(X_train, y_train, epochs=10, \n",
    "#                     validation_data=(X_val, y_val), \n",
    "#                     verbose=0)\n",
    "\n",
    "# # Create a figure and axis object\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# # Plot the training and validation accuracy\n",
    "# ax[0].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "# ax[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# ax[0].set_title('Model Accuracy')\n",
    "# ax[0].set_ylabel('Accuracy')\n",
    "# ax[0].set_xlabel('Epoch')\n",
    "# ax[0].legend()\n",
    "\n",
    "# # Plot the training and validation loss\n",
    "# ax[1].plot(history.history['loss'], label='Training Loss')\n",
    "# ax[1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "# ax[1].set_title('Model Loss')\n",
    "# ax[1].set_ylabel('Loss')\n",
    "# ax[1].set_xlabel('Epoch')\n",
    "# ax[1].legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# In this example, we're creating a figure with two subplots using plt.subplots. The first subplot shows the training and validation accuracy, and the second subplot shows the training and validation loss.\n",
    "\n",
    "# We're using the plot function to create the lines for each metric, and adding labels, titles, and legends to make the plot informative. Finally, we're using plt.tight_layout to ensure the subplots fit nicely in the figure, and plt.show to display the plot.\n",
    "\n",
    "# This will generate a plot that shows the training history of the model, with accuracy and loss curves for both the training and validation sets. This can help you visualize the model's performance over time and identify trends, such as overfitting or convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q16. Evaluate the model's performance using the test dataset and report relevant metrics.\n",
    "# # Answer :\n",
    "# To evaluate the model's performance using the test dataset and report relevant metrics, you can use various evaluation metrics depending on the type of problem you're solving. Here are a few examples:\n",
    "\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# # For classification problems:\n",
    "# y_pred = model.predict(X_test)\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# # For regression problems:\n",
    "# y_pred = model.predict(X_test)\n",
    "# print(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_pred))\n",
    "# print(\"R-squared (R²):\", r2_score(y_test, y_pred))\n",
    "# In the above code:\n",
    "\n",
    "# For classification problems, we use:\n",
    "# accuracy_score to calculate the accuracy of the model.\n",
    "# classification_report to generate a classification report that includes precision, recall, F1-score, and support for each class.\n",
    "# confusion_matrix to generate a confusion matrix that displays the number of true positives, false positives, true negatives, and false negatives.\n",
    "# For regression problems, we use:\n",
    "# mean_squared_error to calculate the mean squared error (MSE) between the predicted and actual values.\n",
    "# r2_score to calculate the R-squared (R²) value, which measures the goodness of fit of the model.\n",
    "# You can choose the relevant metrics based on the problem you're solving and the type of data you're working with. These metrics will provide insights into the model's performance and help you evaluate its efficacy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
