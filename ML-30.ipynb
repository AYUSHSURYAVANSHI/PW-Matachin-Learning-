{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is boosting in machine learning?\n",
    "# Answer :-Boosting is a machine learning ensemble technique that combines the predictions of multiple weak learners to create a strong learner. The primary idea behind boosting is to sequentially train models, with each subsequent model focusing on correcting the errors of the previous ones. The final model is a weighted sum of the individual weak learners.\n",
    "\n",
    "# Here are the key concepts associated with boosting:\n",
    "\n",
    "# Weak Learners: These are models that perform slightly better than random chance but are not necessarily strong on their own. Common examples include shallow decision trees or linear models.\n",
    "\n",
    "# Sequential Training: Boosting trains a series of weak learners sequentially. Each new model is trained to correct the errors made by the combination of all the existing models.\n",
    "\n",
    "# Weighted Training Instances: Instances that are misclassified by the previous models are given higher weights, emphasizing the importance of getting those instances correct in the subsequent models.\n",
    "\n",
    "# Weighted Sum of Predictions: The final prediction is a weighted sum of the predictions of all weak learners. The weights are determined based on the performance of each weak learner, giving more influence to those that perform well.\n",
    "\n",
    "# Adaptive Learning: Boosting is adaptive in nature. It adapts its focus to the instances that are difficult to classify, improving the overall model performance.\n",
    "\n",
    "# Common boosting algorithms include AdaBoost (Adaptive Boosting), Gradient Boosting, and XGBoost (Extreme Gradient Boosting). Each of these algorithms has variations and optimizations, but the core concept of boosting remains the same.\n",
    "\n",
    "# Boosting is powerful and often yields high accuracy models. However, it is important to be mindful of overfitting, especially when using complex models in the ensemble. Regularization techniques and careful hyperparameter tuning are often employed to address this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What are the advantages and limitations of using boosting techniques?\n",
    "# Answer :-\n",
    "# Boosting techniques offer several advantages, but they also come with some limitations. Let's explore both aspects:\n",
    "\n",
    "# Advantages:\n",
    "# Improved Accuracy:\n",
    "\n",
    "# Boosting often results in highly accurate models. By combining the predictions of multiple weak learners, boosting can effectively capture complex patterns in the data.\n",
    "# Handles Weak Learners:\n",
    "\n",
    "# Boosting is designed to work with weak learners, such as shallow decision trees. It can convert a collection of weak models into a strong model, making it versatile for various types of base learners.\n",
    "# Adaptability:\n",
    "\n",
    "# Boosting is adaptive and focuses on instances that are difficult to classify. It continually adjusts its strategy based on the performance of previous models, making it effective in handling complex relationships in the data.\n",
    "# Reduces Overfitting:\n",
    "\n",
    "# Boosting algorithms, especially when used with proper regularization techniques, can help mitigate overfitting. The sequential training process with weighted instances tends to focus on misclassified examples, reducing the risk of overfitting to noise in the data.\n",
    "# Feature Importance:\n",
    "\n",
    "# Boosting algorithms often provide insights into feature importance. They can highlight which features are more influential in making accurate predictions.\n",
    "# Limitations:\n",
    "# Sensitivity to Noisy Data:\n",
    "\n",
    "# Boosting algorithms can be sensitive to noisy data and outliers, especially if not enough emphasis is given to handle them during training. Noisy instances can be assigned higher weights and influence the model disproportionately.\n",
    "# Computational Complexity:\n",
    "\n",
    "# Boosting algorithms, especially as the number of weak learners increases, can be computationally expensive and time-consuming. Training a large number of trees sequentially may not be suitable for real-time applications.\n",
    "# Potential Overfitting:\n",
    "\n",
    "# While boosting aims to reduce overfitting, there is still a risk of overfitting, particularly if the base learners are too complex or if the boosting process continues for too many iterations.\n",
    "# Parameter Sensitivity:\n",
    "\n",
    "# Boosting algorithms often have several hyperparameters that need careful tuning. They can be sensitive to the choice of learning rate, number of iterations, and the depth of individual trees.\n",
    "# Interpretability:\n",
    "\n",
    "# Boosting models can be less interpretable compared to simpler models like decision trees. The combination of many weak learners can make it challenging to understand the decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Explain how boosting works.\n",
    "# Answer :-\n",
    "\n",
    "# Boosting is an ensemble learning technique that combines the predictions of multiple weak learners to create a strong learner. The primary idea behind boosting is to sequentially train models, with each subsequent model focusing on correcting the errors made by the combination of the existing models. The final prediction is a weighted sum of the individual weak learner predictions.\n",
    "\n",
    "# Here's a step-by-step explanation of how boosting works:\n",
    "\n",
    "# Initialize Weights:\n",
    "\n",
    "# Assign equal weights to all instances in the training dataset. These weights determine the importance of each instance during training.\n",
    "# Train a Weak Learner:\n",
    "\n",
    "# Train a weak learner (e.g., a shallow decision tree) on the training data. The weak learner aims to make predictions, but since it's a simple model, its accuracy may be limited.\n",
    "# Compute Error:\n",
    "\n",
    "# Evaluate the performance of the weak learner on the training data. Identify the instances that are misclassified, and assign higher weights to those instances.\n",
    "# Compute Learner Weight:\n",
    "\n",
    "# Calculate the weight of the weak learner based on its accuracy. More accurate learners are given higher weights in the final combination.\n",
    "# Update Instance Weights:\n",
    "\n",
    "# Update the weights of instances in the training data. Instances that were misclassified by the weak learner now have higher weights, making them more influential in the next iteration.\n",
    "# Repeat:\n",
    "\n",
    "# Repeat the process by training another weak learner. The new learner focuses on the instances that were challenging for the previous models. This process is repeated for a predefined number of iterations or until a certain level of performance is achieved.\n",
    "# Combine Weak Learners:\n",
    "\n",
    "# Combine the predictions of all weak learners. The combination is typically a weighted sum, where the weights are determined by the accuracy of each weak learner.\n",
    "# Final Prediction:\n",
    "\n",
    "# The final prediction is made based on the combined predictions of all weak learners. In classification tasks, 'hard' voting (majority vote) or 'soft' voting (weighted average of probabilities) is often used.\n",
    "# The boosting process adapts its focus to instances that are difficult to classify, gradually improving the overall model performance. The sequential training and adaptive weighting of instances distinguish boosting from other ensemble methods. Common boosting algorithms include AdaBoost, Gradient Boosting (e.g., with libraries like XGBoost, LightGBM, or CatBoost), and others. Each iteration of boosting corrects the mistakes of the previous models, resulting in a strong and accurate ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What are the different types of boosting algorithms?\n",
    "# Answer :-\n",
    "# There are several boosting algorithms, each with its own variations and optimizations. Some of the most widely used boosting algorithms include:\n",
    "\n",
    "# AdaBoost (Adaptive Boosting):\n",
    "\n",
    "# AdaBoost is one of the earliest and most popular boosting algorithms. It assigns weights to instances in the dataset, focusing more on the misclassified instances in each iteration. Subsequent weak learners are trained to correct the errors of the previous ones.\n",
    "# Gradient Boosting Machines (GBM):\n",
    "\n",
    "# Gradient Boosting is a general term for boosting algorithms that optimize a differentiable loss function. The most common implementation of GBM is the gradient boosting decision trees. Examples include:\n",
    "# XGBoost (Extreme Gradient Boosting): A scalable and efficient implementation of gradient boosting.\n",
    "# LightGBM: A gradient boosting framework that uses tree-based learning algorithms.\n",
    "# CatBoost: A boosting algorithm that is designed to handle categorical features efficiently.\n",
    "# Stochastic Gradient Boosting:\n",
    "\n",
    "# This is a variant of gradient boosting that introduces randomness in the training process. It involves training each tree on a random subset of the data, providing diversity and reducing overfitting.\n",
    "# LogitBoost:\n",
    "\n",
    "# LogitBoost is a boosting algorithm specifically designed for binary classification problems. It minimizes a logistic loss function and updates the model using a Newton-Raphson optimization technique.\n",
    "# BrownBoost:\n",
    "\n",
    "# BrownBoost is an adaptive boosting algorithm that focuses on instances that are difficult to classify. It incorporates the concept of a margin to adjust the weights of the instances.\n",
    "# LPBoost (Linear Programming Boosting):\n",
    "\n",
    "# LPBoost is a boosting algorithm that minimizes a linear combination of a loss function and a regularization term. It is particularly useful when the weak learners are linear models.\n",
    "# TotalBoost:\n",
    "\n",
    "# TotalBoost is a boosting algorithm that combines AdaBoost and TotalBoost to improve accuracy. It incorporates both additive and multiplicative updates to the weights of the training instances.\n",
    "# MadaBoost (Margin Adaptive Boosting):\n",
    "\n",
    "# MadaBoost is an extension of AdaBoost that considers the margin of the weak learners. It adjusts the weights of the instances based on the margin to improve the performance.\n",
    "# These boosting algorithms share the common concept of sequentially training weak learners and adjusting instance weights to focus on misclassified instances. The specific techniques and optimizations vary, and the choice of the algorithm may depend on the characteristics of the dataset and the problem at hand. XGBoost, LightGBM, and CatBoost, in particular, are widely used in practice due to their efficiency and effectiveness across a range of applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What are some common parameters in boosting algorithms?\n",
    "# Answer :-\n",
    "# Boosting algorithms, such as AdaBoost, Gradient Boosting Machines (GBM), XGBoost, LightGBM, and CatBoost, have various parameters that can be tuned to optimize the performance of the models. Here are some common parameters found in boosting algorithms:\n",
    "\n",
    "# Number of Estimators (n_estimators):\n",
    "\n",
    "# The number of weak learners (trees) to be trained in the ensemble. Increasing the number of estimators may improve performance but also increases computation time.\n",
    "# Learning Rate (or Shrinkage) (learning_rate):\n",
    "\n",
    "# A factor by which the contributions of each weak learner are scaled. Lower values of learning rate usually require more estimators to achieve the same performance but can improve generalization.\n",
    "# Tree-Specific Parameters (max_depth, min_samples_split, min_samples_leaf):\n",
    "\n",
    "# Parameters that control the structure of the individual decision trees in the ensemble. Adjusting these parameters can help prevent overfitting and control the complexity of the trees.\n",
    "# Subsample:\n",
    "\n",
    "# The fraction of training instances used for training each weak learner. Setting it to less than 1.0 introduces randomness and can help prevent overfitting.\n",
    "# Loss Function:\n",
    "\n",
    "# The loss function to be optimized during training. Different boosting algorithms may have different default loss functions, and users can sometimes customize them.\n",
    "# Column (Feature) Sampling:\n",
    "\n",
    "# The fraction of features (columns) used for training each weak learner. It introduces randomness and can help prevent overfitting, especially when dealing with a large number of features.\n",
    "# Scale Pos Weight (or scale_pos_weight):\n",
    "\n",
    "# Used in binary classification problems to control the balance of positive and negative weights. It is particularly useful when dealing with imbalanced datasets.\n",
    "# Regularization Parameters (reg_alpha, reg_lambda):\n",
    "\n",
    "# L1 (reg_alpha) and L2 (reg_lambda) regularization terms that penalize the complexity of the weak learners. They help prevent overfitting.\n",
    "# Gamma:\n",
    "\n",
    "# A parameter that controls the minimum reduction in the loss required for a node to split. It influences the level of tree pruning and can help prevent overfitting.\n",
    "# Max Delta Step (max_delta_step):\n",
    "\n",
    "# A parameter that limits the step size when updating the weights during training. It can be used to make the optimization more conservative.\n",
    "# Objective Function (objective):\n",
    "\n",
    "# Specifies the learning task and the corresponding objective function. Common options include 'reg:squarederror' for regression problems and 'binary:logistic' for binary classification.\n",
    "# Tree Method (tree_method):\n",
    "\n",
    "# The method used to build weak learners. Options may include 'auto,' 'exact,' 'approx,' or 'hist' depending on the boosting algorithm.\n",
    "# These parameters provide flexibility in configuring boosting algorithms based on the characteristics of the dataset and the specific problem being addressed. Proper tuning of these parameters is crucial for achieving optimal model performance. Grid search or randomized search can be used to systematically explore different combinations of parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. How do boosting algorithms combine weak learners to create a strong learner?\n",
    "# Answer :-\n",
    "# Boosting algorithms combine weak learners to create a strong learner through a process of sequential training and weighted voting. The primary idea is to iteratively train weak learners, giving more emphasis to instances that were misclassified by previous models. The combination of these weak learners is done through weighted aggregation of their predictions. The following steps explain how boosting algorithms achieve this:\n",
    "\n",
    "# Initialization:\n",
    "\n",
    "# Assign equal weights to all training instances. These weights determine the importance of each instance during the training process.\n",
    "# Sequential Training:\n",
    "\n",
    "# Train a weak learner on the training data. The weak learner aims to make predictions, but due to its simplicity, its accuracy may be limited.\n",
    "# Compute Error:\n",
    "\n",
    "# Evaluate the performance of the weak learner on the training data. Identify the instances that are misclassified, and assign higher weights to those instances.\n",
    "# Compute Learner Weight:\n",
    "\n",
    "# Calculate the weight of the weak learner based on its accuracy. More accurate learners are given higher weights in the final combination.\n",
    "# Update Instance Weights:\n",
    "\n",
    "# Update the weights of instances in the training data. Instances that were misclassified by the weak learner now have higher weights, making them more influential in the next iteration.\n",
    "# Repeat:\n",
    "\n",
    "# Repeat the process by training another weak learner. The new learner focuses on the instances that were challenging for the previous models. This process is repeated for a predefined number of iterations or until a certain level of performance is achieved.\n",
    "# Combine Predictions:\n",
    "\n",
    "# The final prediction is made by combining the predictions of all weak learners. The combination is typically a weighted sum, where the weights are determined by the accuracy of each weak learner.\n",
    "# For binary classification problems, the final prediction might use 'hard' voting, where the majority vote determines the class. For regression problems, predictions may be combined using a weighted average.\n",
    "\n",
    "# Final Model:\n",
    "\n",
    "# The combination of all weak learners results in a strong learner that is capable of making accurate predictions on the given task. The final model can be used to make predictions on new, unseen data.\n",
    "# The key to boosting's success lies in its ability to adaptively adjust the model's focus to instances that are difficult to classify. By sequentially training weak learners and adjusting instance weights, boosting gradually corrects the errors made by the combination of existing models. This adaptive learning process results in a strong ensemble model that often outperforms individual weak learners. Popular boosting algorithms, such as AdaBoost, Gradient Boosting, XGBoost, LightGBM, and CatBoost, follow these principles with some variations in the specific techniques used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Explain the concept of AdaBoost algorithm and its working.\n",
    "# Answer :-\n",
    "# AdaBoost, short for Adaptive Boosting, is one of the pioneering and widely used boosting algorithms in machine learning. It is an ensemble learning technique that combines the predictions of multiple weak learners to create a strong learner. AdaBoost focuses on sequentially training weak learners and adjusting instance weights to correct the errors made by the combination of existing models. The following steps outline the working of the AdaBoost algorithm:\n",
    "\n",
    "# Initialization:\n",
    "\n",
    "# Assign equal weights to all training instances. These weights represent the importance of each instance during the training process.\n",
    "# Sequential Training of Weak Learners:\n",
    "\n",
    "# Train a weak learner (e.g., a shallow decision tree) on the training data. The weak learner is trained to classify instances but may not perform well on its own.\n",
    "# Compute Error:\n",
    "\n",
    "# Evaluate the performance of the weak learner on the training data. Identify the instances that are misclassified, and compute the error.\n",
    "# Calculate Learner Weight:\n",
    "\n",
    "# Calculate the weight of the weak learner based on its accuracy in classifying instances. More accurate learners receive higher weights.\n",
    "# Update Instance Weights:\n",
    "\n",
    "# Update the weights of instances in the training data. Increase the weights of instances that were misclassified by the weak learner, making them more influential in the next iteration.\n",
    "# Repeat:\n",
    "\n",
    "# Repeat the process by training another weak learner. The new learner focuses on the instances that were challenging for the previous models. This process is repeated for a predefined number of iterations or until a certain level of performance is achieved.\n",
    "# Combine Predictions:\n",
    "\n",
    "# Combine the predictions of all weak learners to make the final prediction. Each weak learner contributes to the final prediction with a weight proportional to its accuracy. For binary classification, 'hard' voting is often used.\n",
    "# Final Model:\n",
    "\n",
    "# The combination of all weak learners results in a strong ensemble model. The final model can be used to make predictions on new, unseen data.\n",
    "# The adaptive nature of AdaBoost lies in its ability to adjust the weights of instances during training, giving more importance to instances that were difficult to classify by previous weak learners. This adaptability helps AdaBoost focus on the most challenging instances, gradually improving the overall model's performance.\n",
    "\n",
    "# AdaBoost is particularly effective in situations where weak learners have performance slightly better than random chance. It has been successfully applied to various machine learning tasks, including classification and regression. However, AdaBoost is sensitive to noisy data and outliers, and careful parameter tuning is essential for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. What is the loss function used in AdaBoost algorithm?\n",
    "# Answer :-\n",
    "# The AdaBoost algorithm does not use a traditional loss function like those employed in some other machine learning algorithms. Instead, it relies on the concept of an exponential loss function to adaptively assign weights to instances during training.\n",
    "\n",
    "# The exponential loss function used in AdaBoost can be defined as follows:\n",
    "\n",
    "# L(y,f(x))=e −yf(x)\n",
    " \n",
    "\n",
    "# Where:\n",
    "\n",
    "# L is the loss function.\n",
    "\n",
    "# y is the true class label (\n",
    "\n",
    "# +1 or −1 for binary classification).\n",
    "\n",
    "# f(x) is the weighted sum of weak learner predictions.\n",
    "# The goal of AdaBoost is to minimize the exponential loss function by adjusting the weights of instances and the contributions of weak learners. The weight assigned to each weak learner is determined by its error rate, i.e., how often it misclassifies instances. More accurate weak learners receive higher weights in the final combination.\n",
    "\n",
    "# In each iteration, AdaBoost identifies the instances that are misclassified by the current weak learner and increases their weights. The subsequent weak learner is then trained to focus on these challenging instances. This adaptability allows AdaBoost to sequentially correct errors made by the combination of weak learners.\n",
    "\n",
    "# While the exponential loss function is central to the training mechanism of AdaBoost, it's important to note that AdaBoost itself is not limited to a specific loss function. The choice of the exponential loss is a design decision that facilitates the adaptive boosting process. The final prediction in AdaBoost is determined by a weighted sum of the individual weak learner predictions, with weights based on their accuracy in reducing the exponential loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. How does the AdaBoost algorithm update the weights of misclassified samples?\n",
    "# Answer :-\n",
    "# In the AdaBoost algorithm, the weights of misclassified samples are updated in each iteration to give more importance to the instances that were difficult for the current weak learner to classify correctly. The process of updating weights is a crucial aspect of AdaBoost's adaptive learning mechanism. Here's how the weights are updated:\n",
    "\n",
    "# Let's assume we have a binary classification problem with class labels \n",
    "# yi(+1 or −1) and predictions \n",
    "# f(xi) from the weak learner for each instance i.\n",
    "\n",
    "# Compute Weighted Error (misclassification rate):\n",
    "\n",
    "# Calculate the weighted error (err t ) of the weak learner at iteration \n",
    "\n",
    "# t as the sum of weights assigned to misclassified instances:\n",
    "\n",
    "# err t= \n",
    "# ∑ i=1 N wi,t⋅I(yi=ft(xi))/∑ i=1 N wi,t\n",
    "  \n",
    "# where \n",
    "\n",
    "# N is the number of training instances, \n",
    "# wi,t is the weight of instance \n",
    "# �\n",
    "# i at iteration \n",
    "# �\n",
    "# t, and \n",
    "\n",
    "# I is the indicator function (1 if true, 0 if false).\n",
    "# Compute Weak Learner Weight (αt):\n",
    "\n",
    "# Calculate the weight (αt) assigned to the weak learner at iteration \n",
    "# t based on its accuracy in reducing the weighted error:\n",
    "\n",
    "# αt= 1/2 ln(errt/1−errt)\n",
    "# Update Instance Weights:\n",
    "\n",
    "# Update the weights of instances for the next iteration (\n",
    "\n",
    "# wi,t+1=wi,t⋅exp(−α t⋅yi⋅f t(xi))\n",
    "# This update formula increases the weights of misclassified instances and decreases the weights of correctly classified instances. The term \n",
    "\n",
    "# exp(−αt⋅yi⋅f t(xi)) acts as a weight multiplier for each instance.\n",
    "\n",
    "# Normalize Instance Weights:\n",
    "\n",
    "# Normalize the updated weights so that they sum to 1:\n",
    "\n",
    "# wi,t+1 = ∑i=1 N wi,t+1 wi,t+1\n",
    "\n",
    "\n",
    "# These steps are repeated for a predefined number of iterations or until a certain stopping criterion is met. The final prediction is then made by combining the weighted predictions of all weak learners.\n",
    "\n",
    "# The adaptive nature of AdaBoost is reflected in this weight update process, where instances that are misclassified by the current weak learner receive higher weights, making them more influential in the subsequent training of the next weak learner. This adaptability allows AdaBoost to focus on instances that are challenging for the ensemble, gradually improving its overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?\n",
    "# Answer :-\n",
    "\n",
    "# Increasing the number of estimators (weak learners) in the AdaBoost algorithm typically has both positive and negative effects, and the impact may vary based on the specific characteristics of the dataset and the problem at hand. Here are some key effects:\n",
    "\n",
    "# Positive Effects:\n",
    "# Improved Accuracy:\n",
    "\n",
    "# In general, increasing the number of estimators often leads to better accuracy, as the ensemble has more opportunities to correct errors and capture complex patterns in the data.\n",
    "# Better Generalization:\n",
    "\n",
    "# A larger number of estimators can enhance the generalization ability of the AdaBoost model, making it more robust to variations in the training data and improving its performance on unseen data.\n",
    "# Reduced Overfitting:\n",
    "\n",
    "# AdaBoost has an inherent ability to adaptively adjust weights and focus on difficult-to-classify instances. Increasing the number of estimators can further mitigate overfitting, especially if the weak learners are not too complex.\n",
    "# Negative Effects:\n",
    "# Increased Computational Cost:\n",
    "\n",
    "# Training more weak learners requires more computational resources and time. The training time of AdaBoost is proportional to the number of estimators, so a large number of estimators may result in longer training times.\n",
    "# Diminishing Returns:\n",
    "\n",
    "# There may be diminishing returns in terms of performance improvement beyond a certain number of estimators. After reaching a certain point, the additional weak learners might not significantly contribute to better performance.\n",
    "# Risk of Overfitting:\n",
    "\n",
    "# While AdaBoost is less prone to overfitting compared to some other algorithms, there is still a risk of overfitting, especially if the number of estimators is excessively large and the weak learners are too complex.\n",
    "# Guiding Principles:\n",
    "# Cross-Validation:\n",
    "\n",
    "# It is advisable to use cross-validation to find an optimal number of estimators that balances model performance and computational efficiency.\n",
    "# Early Stopping:\n",
    "\n",
    "# Implement early stopping criteria to halt the training process when performance on a validation set stops improving. This can prevent overfitting and reduce unnecessary computation.\n",
    "# Regularization:\n",
    "\n",
    "# If overfitting becomes a concern with a large number of estimators, consider adding regularization techniques or constraints to the weak learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
