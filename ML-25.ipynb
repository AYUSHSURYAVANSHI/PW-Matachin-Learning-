{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "# company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "# probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "# Answer :-\n",
    "# To find the probability that an employee is a smoker given that he/she uses the health insurance plan, you can use conditional probability. The conditional probability of event A (being a smoker) given event B (using the health insurance plan) is denoted by P(A|B) and is calculated using the formula:\n",
    "\n",
    "# P(A∣B)= \n",
    "# P(B)\n",
    "# P(A∩B)\n",
    "\n",
    "# In this case:\n",
    "\n",
    "# Event A is being a smoker.\n",
    "# Event B is using the health insurance plan.\n",
    "# You're given:\n",
    "\n",
    "\n",
    "# P(B), the probability of using the health insurance plan, which is 70% (0.7).\n",
    "\n",
    "# P(A∩B), the probability of being a smoker and using the health insurance plan, which is 40% of the employees who use the plan, i.e., 40% of 70% (0.4 * 0.7).\n",
    "# Now, substitute these values into the formula:\n",
    "\n",
    "\n",
    "# P(A∣B)= \n",
    "# 0.7\n",
    "# 0.4×0.7\n",
    "\n",
    "# Simplify the expression:\n",
    "\n",
    "# P(A∣B)=0.4\n",
    "\n",
    "# So, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.4 or 40%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "# Answer :-\n",
    "# Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes algorithm, which is a probabilistic classification algorithm based on Bayes' theorem. The main difference between them lies in the type of data they are designed to handle.\n",
    "\n",
    "# Bernoulli Naive Bayes:\n",
    "\n",
    "# Data Type: It is suitable for binary data, where each feature represents a binary variable (0 or 1).\n",
    "# Use Case: Commonly used in text classification tasks, where the presence or absence of a word in a document is considered.\n",
    "# Multinomial Naive Bayes:\n",
    "\n",
    "# Data Type: It is designed for discrete data, typically for cases where features represent the frequency of occurrences (counts) of events.\n",
    "# Use Case: Widely used in text classification, particularly when the features are the word counts (or term frequencies) within documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "# Answer :-\n",
    "# Bernoulli Naive Bayes handles missing values by ignoring them during the training process. In the context of Bernoulli Naive Bayes, which is commonly used for binary data (features that are either present or absent), missing values are treated as if the corresponding features are absent.\n",
    "\n",
    "# The underlying assumption of the Naive Bayes algorithm, including Bernoulli Naive Bayes, is that features are conditionally independent given the class label. When a feature is missing for a particular instance, the algorithm simply excludes that feature from consideration for that instance when calculating probabilities.\n",
    "\n",
    "# During training, the algorithm estimates probabilities based on the available features. If a feature is missing for a particular instance, the algorithm does not take that feature into account when updating its probability estimates. This approach is consistent with the \"naive\" assumption of independence among features.\n",
    "\n",
    "# During classification or prediction, when the algorithm encounters a missing value for a feature, it simply ignores that feature when calculating the likelihoods and makes predictions based on the available features.\n",
    "\n",
    "# It's important to note that the handling of missing values in Bernoulli Naive Bayes is inherently built into the algorithm's design, and there is no need for explicit imputation or special treatment of missing values during the training or prediction phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "# Answer :-\n",
    "# Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is an extension of the Naive Bayes algorithm that is designed to handle continuous data. It assumes that the features follow a Gaussian (normal) distribution within each class.\n",
    "\n",
    "# In the context of multi-class classification, where there are more than two classes, Gaussian Naive Bayes can still be applied. The algorithm calculates the likelihood of the observed data given each class using the Gaussian distribution parameters (mean and variance) for each feature within each class.\n",
    "\n",
    "# The decision rule for classification involves selecting the class with the highest posterior probability, which is the product of the prior probability of the class and the likelihood of the observed data given that class. The prior probability represents the probability of each class without considering the features.\n",
    "\n",
    "# To summarize, Gaussian Naive Bayes can handle multiple classes by extending its calculations for the Gaussian distribution parameters and applying the standard Naive Bayes classification decision rule. Each class is treated independently, and the class with the highest posterior probability is predicted for a given set of feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Assignment:\n",
    "# Data preparation:\n",
    "# Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "# datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "# is spam or not based on several input features.\n",
    "# Implementation:\n",
    "# Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "# scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "# dataset. You should use the default hyperparameters for each classifier.\n",
    "# Results:\n",
    "# Report the following performance metrics for each classifier:\n",
    "# Accuracy\n",
    "# Precision\n",
    "# Recall\n",
    "# F1 score\n",
    "# Discussion:\n",
    "# Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "# the case? Are there any limitations of Naive Bayes that you observed?\n",
    "# Conclusion:\n",
    "# Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "# Note: This dataset contains a binary classification problem with multiple features. The dataset is\n",
    "# relatively small, but it can be used to demonstrate the performance of the different variants of Naive\n",
    "# Bayes on a real-world problem.\n",
    "# Answer :-\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "column_names = [...]  # Add column names based on the dataset description\n",
    "\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "data = pd.read_csv(url, header=None, names=column_names)\n",
    "\n",
    "X = data.drop('target_variable_column_name', axis=1)  # Replace 'target_variable_column_name' with the actual column name\n",
    "y = data['target_variable_column_name']\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Create Naive Bayes classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Perform 10-fold cross-validation and evaluate performance metrics\n",
    "def evaluate_classifier(classifier, X, y):\n",
    "    accuracy = cross_val_score(classifier, X, y, cv=10, scoring='accuracy').mean()\n",
    "    precision = cross_val_score(classifier, X, y, cv=10, scoring='precision').mean()\n",
    "    recall = cross_val_score(classifier, X, y, cv=10, scoring='recall').mean()\n",
    "    f1 = cross_val_score(classifier, X, y, cv=10, scoring='f1').mean()\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Evaluate each classifier\n",
    "accuracy_b, precision_b, recall_b, f1_b = evaluate_classifier(bernoulli_nb, X, y)\n",
    "accuracy_m, precision_m, recall_m, f1_m = evaluate_classifier(multinomial_nb, X, y)\n",
    "accuracy_g, precision_g, recall_g, f1_g = evaluate_classifier(gaussian_nb, X, y)\n",
    "\n",
    "# Print the results\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "print(f\"Accuracy: {accuracy_b}\")\n",
    "print(f\"Precision: {precision_b}\")\n",
    "print(f\"Recall: {recall_b}\")\n",
    "print(f\"F1 Score: {f1_b}\")\n",
    "\n",
    "print(\"\\nMultinomial Naive Bayes:\")\n",
    "print(f\"Accuracy: {accuracy_m}\")\n",
    "print(f\"Precision: {precision_m}\")\n",
    "print(f\"Recall: {recall_m}\")\n",
    "print(f\"F1 Score: {f1_m}\")\n",
    "\n",
    "print(\"\\nGaussian Naive Bayes:\")\n",
    "print(f\"Accuracy: {accuracy_g}\")\n",
    "print(f\"Precision: {precision_g}\")\n",
    "print(f\"Recall: {recall_g}\")\n",
    "print(f\"F1 Score: {f1_g}\")\n",
    "\n",
    "# Step 4: Discussion and Conclusion\n",
    "# Discuss the results, compare the performance of the three Naive Bayes variants, and highlight any observed limitations. Conclude with a summary of your findings and suggestions for future work.\n",
    "\n",
    "# Note: Ensure that you replace 'target_variable_column_name' with the actual name of the target variable column in your dataset. Additionally, you may need to adjust column names and dataset specifics based on the actual dataset structure and features."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
